# ResearchPro Advanced RAG System - Architecture Summary

## Project Overview

**ResearchPro** is an advanced Retrieval-Augmented Generation (RAG) system designed for academic research paper analysis. It combines multimodal document processing (text, tables, images) with hybrid retrieval strategies and conversational AI to provide intelligent question-answering over research documents.

---

## Architecture Overview

```mermaid
graph TB
    User[User - Streamlit Frontend]
    API[FastAPI Backend]
    
    User -->|Upload PDF| API
    User -->|Send Query| API
    
    subgraph Frontend
        ST[Streamlit App]
        Session[Session State Manager]
    end
    
    subgraph Backend
        Main[main.py - API Endpoints]
        
        subgraph Services
            DocProc[DocumentProcessor]
            Vision[MultimodalProcessor]
            RAG[RAG_Pipeline]
            Rerank[ReRanker_Model]
        end
        
        subgraph Utils
            SessMan[SessionManager]
        end
        
        subgraph Config
            Cfg[config.py - Models & Settings]
        end
    end
    
    subgraph External
        GROQ[Groq API - LLMs]
        HF[HuggingFace - Embeddings]
    end
    
    Main --> DocProc
    Main --> RAG
    Main --> Rerank
    Main --> SessMan
    DocProc --> Vision
    DocProc --> Cfg
    RAG --> Cfg
    Rerank --> Cfg
    
    Cfg --> GROQ
    Cfg --> HF
```

---

## Component Breakdown

### 1. **Frontend Layer** ([frontend/streamlit_app.py](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/frontend/streamlit_app.py))

**Role:** User interface for document upload and conversational querying

**Responsibilities:**
- Render chat interface with message history
- Handle PDF file uploads via file uploader widget
- Manage session state (session_id, chat messages, upload status)
- Make HTTP requests to FastAPI backend
- Display API connection status
- Provide controls for clearing chat/vectorstore

**Key Features:**
- Session-based conversation tracking using UUID
- Real-time API health monitoring
- Chat message persistence within session
- Document reset functionality

---

### 2. **Backend API Layer** ([backend/app/main.py](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/main.py))

**Role:** FastAPI server exposing REST endpoints for document processing and querying

**Responsibilities:**
- Receive and temporarily store uploaded PDF files
- Orchestrate document processing pipeline
- Initialize and manage RAG pipeline components
- Handle query requests with session context
- Manage vectorstore lifecycle (create/delete)
- Error handling and HTTP response formatting

**Component Initialization:**
```python
document_processor = DocumentProcessor()
rag_pipeline = RAG_Pipeline(llm)
reranker = ReRanker_Model(hf_reranker_encoder)
session_manager = SessionManager()
```

---

### 3. **Configuration Layer** ([config/config.py](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/config/config.py))

**Role:** Centralized configuration for all AI models and embeddings

**Models Configured:**
- **`llm`**: ChatGroq with `openai/gpt-oss-20b` - Main conversational model
- **`llm_summarize`**: ChatGroq with `llama-3.1-70b-versatile` - For summarizing multimodal content
- **`vision_model`**: `meta-llama/llama-4-scout-17b-16e-instruct` - For image analysis
- **`hf_embeddings`**: `BAAI/bge-small-en-v1.5` - Standard embeddings for vectorstore
- **`hyde_embedding`**: HypotheticalDocumentEmbedder - Enhanced query embeddings
- **`hf_reranker_encoder`**: `cross-encoder/ms-marco-MiniLM-L-6-v2` - Reranking model
- **`groq_client`**: Groq API client for vision model access

**Performance Optimizations:**
- `TOKENIZERS_PARALLELISM = "false"` - Prevents threading conflicts
- `torch.set_num_threads(4)` - Limits CPU thread usage

---

### 4. **Vision Service** ([backend/app/services/vision_service.py](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/vision_service.py))

**Role:** Multimodal document processing - extracts and analyzes text, tables, and images

**Class:** [MultimodalProcessor](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/vision_service.py#9-243)

**Key Methods:**

#### [load_and_process(filepath: str) -> list[Document]](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/vision_service.py#17-87)
**Purpose:** Main entry point for PDF processing with intelligent hybrid strategy

**Process:**
1. **Fast Scan Phase:**
   - Uses `partition_pdf` with `strategy="fast"` to detect pages with tables/images
   - Identifies pages containing: Table, Image, Figure, Graphic elements
   - Keyword detection: "table", "figure", "chart", "diagram", "plot"

2. **Selective Hi-Res Processing:**
   - Only pages with complex elements get `strategy="hi_res"` treatment
   - Extracts table structure as HTML via `infer_table_structure=True`
   - Extracts image blocks as base64 payloads
   - Merges fast-scan pages with hi-res pages

3. **Chunking:**
   - Uses `chunk_by_title` with:
     - `max_characters=3000`
     - `new_after_n_chars=2400`
     - `combine_text_under_n_chars=500`

4. **Document Conversion:**
   - Converts chunks to LangChain [Document](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py#7-146) objects
   - Attaches metadata for tables and images

#### [describe_image(base64_img: str) -> str](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/vision_service.py#90-135)
**Purpose:** Generate textual descriptions of images using vision model

**Process:**
- Validates image size (max 2MB)
- Checks cache to avoid redundant API calls
- Constructs data URI with proper MIME type
- Calls Groq vision model with prompt: "Describe this image in detail."
- Truncates descriptions to 500 characters
- Caches results for reuse

#### [_convert_chunks_without_summary(chunks) -> list[Document]](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/vision_service.py#140-205)
**Purpose:** Convert chunks to Documents with parallel image processing

**Process:**
1. Collect all images from chunks
2. **Parallel Image Description** (ThreadPoolExecutor with max 4 workers)
3. Build Document objects with metadata:
   - `has_tables`: Boolean flag
   - `original_tables`: List of HTML table strings
   - `has_images`: Boolean flag
   - `original_images`: List of `{base64, description}` dicts
   - `image_description`: List of description strings
   - `page_number`: Source page

#### [_generate_ai_summary(text, tables, images) -> str](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/vision_service.py#208-243)
**Purpose:** Create searchable summaries integrating text, tables, and images

**Prompt Strategy:**
- Extract core findings and methodology
- Convert tables to data statements with exact numbers
- Synthesize insights from image descriptions
- Preserve domain terms and metrics
- Keep under 200 words

---

### 5. **Document Service** ([backend/app/services/document_service.py](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py))

**Role:** Document loading, retriever creation, and multimodal context extraction

**Class:** [DocumentProcessor](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py#7-146)

**Key Methods:**

#### [load_and_process_pdf(filepath: str) -> list[Document]](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py#14-26)
**Purpose:** Load PDF and extract multimodal elements

**Process:**
1. Calls `MultimodalProcessor.load_and_process()`
2. Extracts tables using [_extract_tables_from_docs()](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py#27-42)
3. Extracts images using [_extract_images_from_docs()](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py#43-58)
4. Stores in instance variables: `processed_docs`, `extracted_tables`, `extracted_images`

#### [create_retrievers(docs) -> tuple[semantic_retriever, syntactic_retriever]](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py#62-83)
**Purpose:** Create dual retrieval systems

**Returns:**
1. **Semantic Retriever (FAISS):**
   - Vector similarity search using `hf_embeddings`
   - `search_type="similarity"`
   - `k=5` top results

2. **Syntactic Retriever (BM25):**
   - Keyword-based search
   - Preprocessing: lowercase + split
   - `k=5` top results

**Side Effect:** Initializes `self.vectorstore` with FAISS index

#### [get_table_context(query: str) -> str](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py#87-115)
**Purpose:** Extract table context relevant to user query

**Process:**
1. Extract query keywords (words > 3 characters)
2. Search `extracted_tables` for keyword matches
3. Return formatted context with top 3 tables:
   ```
   === RELEVANT TABLES FROM DOCUMENT ===
   [Table 1 - Page X]
   <table content preview (400 chars)>
   ```

#### [get_image_context(query: str) -> str](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/document_service.py#116-137)
**Purpose:** Extract image descriptions when query involves visuals

**Trigger Keywords:** "figure", "image", "chart", "graph", "diagram", "visual"

**Returns:**
```
=== IMAGE ANALYSIS ===
[Image 1 — Page X]
<image description (400 chars)>
```

---

### 6. **RAG Service** ([backend/app/services/rag_service.py](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py))

**Role:** Orchestrate retrieval, reranking, and conversational question-answering

**Class:** [RAG_Pipeline](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py#9-206)

**Key Methods:**

#### [create_hybrid_retriever(syntactic, semantic) -> EnsembleRetriever](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py#106-110)
**Purpose:** Combine BM25 and FAISS retrievers

**Configuration:**
- Equal weights: `[0.5, 0.5]`
- Merges results from both retrievers

#### [create_rag_chain(retriever) -> RetrievalChain](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py#116-134)
**Purpose:** Build conversational RAG chain with history awareness

**Components:**
1. **History-Aware Retriever:**
   - Uses [reformulation_prompt](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py#26-50) to rewrite queries based on chat history
   - Expands vague references ("it", "they", "the table")
   - Makes questions self-contained

2. **Question-Answer Chain:**
   - Uses [answer_prompt](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py#54-99) with specialized table/data analysis instructions
   - Emphasizes extracting specific numbers and trends
   - Requires citing page numbers

#### [create_conversational_chain(rag_chain, get_session_history_func)](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py#136-146)
**Purpose:** Wrap RAG chain with session-based message history

**Configuration:**
- `input_messages_key="input"`
- `history_messages_key="chat_history"`
- `output_messages_key="answer"`

#### [query(question: str, session_id: str) -> str](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py#148-206)
**Purpose:** Main query processing pipeline

**Sequential Process:**

1. **Retrieve Documents:**
   ```python
   retrieved_docs = self.compression_retriever.get_relevant_documents(question)
   top_k = retrieved_docs[:3]
   ```

2. **Summarize Multimodal Chunks:**
   - For docs with tables/images: Generate AI summary (cached by page)
   - For text-only docs: Use raw `page_content`
   - Truncate summaries to 600 chars

3. **Build Enhanced Context:**
   ```python
   summarized_context = "\n\n".join(summarized)
   enhanced_input = f"{question}\n\nSUMMARIZED CONTEXT:\n{summarized_context}"
   ```

4. **Inject Table Context:**
   ```python
   table_context = self.document_processor.get_table_context(question)
   if table_context:
       enhanced_input += f"\n{table_context}"
   ```

5. **Inject Image Context:**
   ```python
   image_context = self.document_processor.get_image_context(question)
   if image_context:
       enhanced_input += f"\n{image_context}"
   ```

6. **Invoke Conversational RAG:**
   ```python
   response = self.conversational_rag.invoke(
       {"input": enhanced_input},
       config={"configurable": {"session_id": session_id}}
   )
   return response.get("answer")
   ```

**Prompts:**

**Reformulation Prompt:**
- Rewrites queries to be self-contained
- Expands abbreviations using chat history
- Makes data/table references explicit

**Answer Prompt:**
- Specialized for academic paper analysis
- Emphasizes table/chart interpretation
- Requires specific data citations
- Structured response format

---

### 7. **Reranker Service** ([backend/app/services/reranker.py](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/reranker.py))

**Role:** Rerank retrieved documents using cross-encoder for relevance

**Class:** [ReRanker_Model](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/reranker.py#5-19)

**Key Method:**

#### [create_compression_retriever(retriever) -> ContextualCompressionRetriever](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/reranker.py#12-19)
**Purpose:** Wrap retriever with reranking compressor

**Configuration:**
- Model: `cross-encoder/ms-marco-MiniLM-L-6-v2`
- `top_n=3` - Return top 3 reranked results
- Base retriever: Hybrid retriever (BM25 + FAISS)

**How It Works:**
1. Base retriever returns candidates (typically 5-10 docs)
2. Cross-encoder scores each doc against query
3. Returns top 3 highest-scoring documents

---

### 8. **Session Manager** ([backend/utils/session_manager.py](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/utils/session_manager.py))

**Role:** Manage conversational chat history per session

**Class:** [SessionManager](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/utils/session_manager.py#5-28)

**Key Methods:**

#### [get_session_history(session_id: str) -> BaseChatMessageHistory](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/utils/session_manager.py#10-15)
**Purpose:** Get or create chat history for a session

**Storage:** In-memory dictionary `self.store`

#### [clear_all_sessions()](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/utils/session_manager.py#25-28)
**Purpose:** Reset all conversation histories

---

## Sequential Workflows

### Workflow 1: Document Upload Process

```mermaid
sequenceDiagram
    participant User
    participant Streamlit
    participant FastAPI
    participant DocProc as DocumentProcessor
    participant Vision as MultimodalProcessor
    participant RAG as RAG_Pipeline
    participant Reranker
    
    User->>Streamlit: Upload PDF file
    Streamlit->>FastAPI: POST /upload_file
    
    FastAPI->>FastAPI: Save to backend/temp/
    FastAPI->>DocProc: load_and_process_pdf(filepath)
    
    DocProc->>Vision: load_and_process(filepath)
    
    Vision->>Vision: Fast scan - detect table/image pages
    Vision->>Vision: Hi-res scan on complex pages
    Vision->>Vision: Chunk by title
    Vision->>Vision: Parallel image description (4 workers)
    Vision-->>DocProc: Return Document objects
    
    DocProc->>DocProc: Extract tables from metadata
    DocProc->>DocProc: Extract images from metadata
    DocProc->>DocProc: create_retrievers(docs)
    DocProc->>DocProc: Create FAISS vectorstore
    DocProc->>DocProc: Create BM25 retriever
    DocProc-->>FastAPI: Return (semantic_retriever, syntactic_retriever)
    
    FastAPI->>RAG: create_hybrid_retriever(syntactic, semantic)
    RAG-->>FastAPI: Return hybrid_retriever
    
    FastAPI->>Reranker: create_compression_retriever(hybrid_retriever)
    Reranker-->>FastAPI: Return compression_retriever
    
    FastAPI->>RAG: set_compression_retriever(compression_retriever)
    FastAPI->>RAG: set_document_processor(document_processor)
    FastAPI->>RAG: update_vectorstore(vectorstore)
    FastAPI->>RAG: create_rag_chain(compression_retriever)
    FastAPI->>RAG: create_conversational_chain(rag_chain, get_session_history)
    
    FastAPI->>FastAPI: Delete temp file
    FastAPI-->>Streamlit: Return success + stats
    Streamlit-->>User: Display "Document ready"
```

**Detailed Steps:**

1. **File Reception (FastAPI):**
   - Receive uploaded file
   - Create `backend/temp/` directory if not exists
   - Save as `temp_{filename}`

2. **Document Processing (DocumentProcessor):**
   - Call `MultimodalProcessor.load_and_process()`
   - Extract tables and images from processed docs
   - Store in instance variables

3. **Multimodal Processing (MultimodalProcessor):**
   - **Fast Scan:** Detect pages with tables/images
   - **Selective Hi-Res:** Process only complex pages with hi_res strategy
   - **Merge:** Combine fast-scan and hi-res results
   - **Chunk:** Split by title with size constraints
   - **Image Analysis:** Parallel processing with 4 workers using vision model
   - **Document Creation:** Build LangChain Documents with rich metadata

4. **Retriever Creation (DocumentProcessor):**
   - **FAISS Vectorstore:** Embed all docs with `hf_embeddings`
   - **BM25 Retriever:** Build keyword index
   - Return both retrievers

5. **RAG Pipeline Setup (main.py orchestration):**
   - **Hybrid Retriever:** Combine BM25 + FAISS (50/50 weights)
   - **Compression Retriever:** Wrap with cross-encoder reranker (top 3)
   - **RAG Chain:** Create history-aware retrieval chain
   - **Conversational Chain:** Wrap with session history management

6. **Cleanup:**
   - Delete temporary file
   - Return statistics (doc count, table count, image count)

---

### Workflow 2: Query Processing

```mermaid
sequenceDiagram
    participant User
    participant Streamlit
    participant FastAPI
    participant RAG as RAG_Pipeline
    participant Reranker
    participant DocProc as DocumentProcessor
    participant LLM
    participant SessionMgr as SessionManager
    
    User->>Streamlit: Enter query
    Streamlit->>FastAPI: POST /query {query, session_id}
    
    FastAPI->>RAG: query(query, session_id)
    
    RAG->>Reranker: compression_retriever.get_relevant_documents(query)
    Reranker->>Reranker: Hybrid retrieval (BM25 + FAISS)
    Reranker->>Reranker: Cross-encoder reranking
    Reranker-->>RAG: Return top 3 docs
    
    RAG->>RAG: Check docs for tables/images
    
    alt Has tables/images
        RAG->>RAG: Check summary_cache
        alt Cache miss
            RAG->>DocProc: multimodal_processor._generate_ai_summary()
            DocProc->>LLM: Summarize text + tables + images
            LLM-->>DocProc: Return summary
            DocProc-->>RAG: Return summary
            RAG->>RAG: Store in summary_cache
        end
    else Text only
        RAG->>RAG: Use page_content directly
    end
    
    RAG->>RAG: Build summarized_context
    RAG->>DocProc: get_table_context(query)
    DocProc-->>RAG: Return relevant tables
    
    RAG->>DocProc: get_image_context(query)
    DocProc-->>RAG: Return image descriptions
    
    RAG->>RAG: Build enhanced_input with all context
    
    RAG->>SessionMgr: get_session_history(session_id)
    SessionMgr-->>RAG: Return chat history
    
    RAG->>LLM: conversational_rag.invoke(enhanced_input, session_id)
    
    LLM->>LLM: Reformulate query with history
    LLM->>LLM: Generate answer with context
    LLM-->>RAG: Return answer
    
    RAG-->>FastAPI: Return answer
    FastAPI-->>Streamlit: Return {response: answer}
    Streamlit-->>User: Display answer in chat
```

**Detailed Steps:**

1. **Query Reception:**
   - User types question in Streamlit chat
   - Frontend sends `{query, session_id}` to `/query` endpoint

2. **Document Retrieval (Reranker):**
   - **Hybrid Retrieval:** BM25 + FAISS both return ~5 candidates each
   - **Ensemble Merge:** Combine with 50/50 weights
   - **Cross-Encoder Reranking:** Score all candidates, return top 3

3. **Context Enhancement (RAG_Pipeline):**
   
   **For each retrieved document:**
   - Check if `has_tables` or `has_images` in metadata
   - If yes:
     - Check `summary_cache` by page number
     - If cache miss: Generate AI summary with `llm_summarize`
     - Cache summary for future queries
     - Truncate to 600 chars
   - If no: Use raw `page_content`

4. **Table Context Injection:**
   - Extract query keywords (words > 3 chars)
   - Search `extracted_tables` for keyword matches
   - Format top 3 tables with page numbers

5. **Image Context Injection:**
   - Check for visual keywords in query
   - If present, include top 3 image descriptions

6. **Enhanced Input Construction:**
   ```
   {query}
   
   SUMMARIZED CONTEXT:
   {summarized_context}
   
   === RELEVANT TABLES FROM DOCUMENT ===
   {table_context}
   
   === IMAGE ANALYSIS ===
   {image_context}
   ```

7. **Conversational Processing:**
   - Retrieve chat history from [SessionManager](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/utils/session_manager.py#5-28)
   - **Reformulation Step:** LLM rewrites query using history
   - **Answer Generation:** LLM generates answer with all context
   - Session history automatically updated

8. **Response Return:**
   - Extract [answer](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/services/rag_service.py#54-99) from response
   - Return to Streamlit
   - Display in chat interface

---

## API Reference

### Base URL
```
http://127.0.0.1:8000
```

---

### **GET /** 
**Description:** Root endpoint with API information

**Response:**
```json
{
  "message": "Welcome to the Advanced Research Assistant",
  "endpoints": {
    "POST /upload_file": "Upload a document for processing",
    "POST /query": "Query the uploaded documents"
  }
}
```

---

### **POST /upload_file**
**Description:** Upload and process a PDF document

**Request:**
- **Content-Type:** `multipart/form-data`
- **Body:**
  - [file](file:///c:/dev/Projects/ResearchPro/ResearchPro_AdvancedRAG/backend/app/main.py#44-104): PDF file (UploadFile)

**Process:**
1. Save file to `backend/temp/temp_{filename}`
2. Load and process PDF (multimodal extraction)
3. Create semantic (FAISS) and syntactic (BM25) retrievers
4. Build hybrid retriever (50/50 ensemble)
5. Apply cross-encoder reranking (top 3)
6. Initialize RAG chain with conversational wrapper
7. Delete temporary file

**Success Response (200):**
```json
{
  "message": "File uploaded and retriever initialized successfully.",
  "stats": {
    "documents": 45,
    "tables": 8,
    "images": 12
  }
}
```

**Error Response (500):**
```json
{
  "detail": "Error processing file: {error_message}"
}
```

---

### **POST /query**
**Description:** Query the processed document with conversational context

**Request:**
- **Content-Type:** `application/json`
- **Body:**
```json
{
  "query": "What are the main findings?",
  "session_id": "optional_session_id"  // defaults to "default_session"
}
```

**Process:**
1. Retrieve top 3 reranked documents
2. Generate/retrieve summaries for multimodal chunks
3. Extract relevant table context
4. Extract relevant image context
5. Build enhanced input with all context
6. Retrieve chat history for session
7. Reformulate query with history
8. Generate answer with LLM
9. Update session history

**Success Response (200):**
```json
{
  "response": "The main findings indicate that..."
}
```

**Error Response (500):**
```json
{
  "detail": "Error processing query: {error_message}"
}
```

---

### **DELETE /delete**
**Description:** Clear vectorstore and all session histories

**Process:**
1. Set `vectorstore = None` in RAG pipeline
2. Set `vectorstore = None` in document processor
3. Clear all retrievers and chains
4. Clear extracted tables and images
5. Clear all session histories

**Success Response (200):**
```json
{
  "message": "Vectorstore and sessions cleared"
}
```

**Error Response (500):**
```json
{
  "detail": "Error clearing vectorstore: {error_message}"
}
```

---

## Key Design Patterns

### 1. **Hybrid Retrieval Strategy**
- **Semantic (FAISS):** Captures conceptual similarity via embeddings
- **Syntactic (BM25):** Captures exact keyword matches
- **Ensemble:** 50/50 weight combination for balanced results

### 2. **Reranking for Precision**
- Cross-encoder scores query-document pairs
- More accurate than bi-encoder embeddings
- Reduces top-k from ~10 to 3 most relevant

### 3. **Multimodal Context Injection**
- Tables and images extracted separately
- Keyword-based relevance matching
- Injected as additional context to LLM

### 4. **Lazy Summarization with Caching**
- Summaries generated only when needed
- Cached by page number to avoid redundant API calls
- Reduces latency for repeated queries

### 5. **Conversational History Management**
- Session-based chat history storage
- Query reformulation using history
- Enables follow-up questions and context retention

### 6. **Selective Hi-Res Processing**
- Fast scan detects complex pages
- Hi-res processing only on pages with tables/images
- Significantly reduces processing time

### 7. **Parallel Image Analysis**
- ThreadPoolExecutor with 4 workers
- Batch processes all images upfront
- Avoids sequential bottleneck

---

## Technology Stack Summary

| Component | Technology |
|-----------|-----------|
| **Frontend** | Streamlit |
| **Backend** | FastAPI |
| **Document Parsing** | Unstructured (partition_pdf) |
| **Vector Database** | FAISS |
| **Keyword Search** | BM25Retriever |
| **Embeddings** | HuggingFace (BAAI/bge-small-en-v1.5) |
| **Reranking** | Cross-Encoder (ms-marco-MiniLM-L-6-v2) |
| **LLM (Main)** | Groq (openai/gpt-oss-20b) |
| **LLM (Summary)** | Groq (llama-3.1-70b-versatile) |
| **Vision Model** | Groq (llama-4-scout-17b-16e-instruct) |
| **Orchestration** | LangChain |
| **Chat History** | LangChain ChatMessageHistory |

---

## Data Flow Summary

### Upload Flow:
```
PDF → Fast Scan → Hi-Res Scan (selective) → Chunking → Image Analysis (parallel) → 
Document Objects → FAISS + BM25 → Hybrid Retriever → Reranker → RAG Chain
```

### Query Flow:
```
Query → Hybrid Retrieval → Reranking (top 3) → Summarization (cached) → 
Table Context → Image Context → Enhanced Input → History Retrieval → 
Query Reformulation → Answer Generation → Response
```

---

## Performance Optimizations

1. **Selective Hi-Res Processing:** Only complex pages get expensive processing
2. **Parallel Image Analysis:** 4 concurrent workers for vision API calls
3. **Summary Caching:** Avoid redundant LLM calls for same pages
4. **Embedding Normalization:** Improves FAISS similarity search accuracy
5. **Thread Limiting:** `torch.set_num_threads(4)` prevents CPU overload
6. **Tokenizer Parallelism Disabled:** Avoids threading conflicts

---

## Session Management

- **Frontend:** UUID-based session IDs generated on app load
- **Backend:** In-memory dictionary stores chat histories
- **Persistence:** Sessions cleared on vectorstore deletion
- **Isolation:** Each session maintains independent conversation context

---

## Error Handling

- **Upload Errors:** Temporary file cleanup on failure
- **Query Errors:** Graceful degradation with error messages
- **API Errors:** HTTP 500 with detailed error descriptions
- **Image Analysis Failures:** Fallback to "[Image could not be analyzed]"
- **Connection Errors:** Frontend displays API disconnected status
